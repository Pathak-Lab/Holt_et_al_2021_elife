{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openpyxl as xl\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import t\n",
    "import plotly.express as px\n",
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Image Paths for Velocity Maps to be read into Python\n",
    "Define a function to take all the velocity heatmap outputs from ADAPT within a folder and then we create a for loop to go through the folder and write images to a dataframe to create heatmaps seen in Figure 5E and Supp Figure 9. The second for loop melts each heatmap into one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    images = sorted(glob(os.path.join(path,\"Velocity Outputs\", \"*.tif\")))\n",
    "    return(images)\n",
    "\n",
    "filepath=\"G:/Shared drives/Keratinocytes/04_Data/Data_Jesse/ADAPT\"\n",
    "filenames = load_data(filepath)\n",
    "\n",
    "for file in filenames:\n",
    "    taco = file\n",
    "    print(taco)\n",
    "    photo = Image.open(taco)\n",
    "    filename = taco.split('/')[-1]\n",
    "    print(filename)\n",
    "    data= np.array(photo) \n",
    "    df = pd.DataFrame(data) \n",
    "    df_1 = df.replace(0, np.nan) \n",
    "    df_1.to_excel('C:/Users/mrjes/Documents/Velocity Outputs/'+filename[:-16]+'_VelocityMapswithNaN.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in filenames:\n",
    "    taco = file\n",
    "    print(taco)\n",
    "    photo = Image.open(taco)\n",
    "    filename = taco.split('/')[-1]\n",
    "    print(filename)\n",
    "    data=np.array(photo) #numpy array of the image\n",
    "    df =pd.DataFrame(data) #puts it into a dataframe\n",
    "    df_1 = df.replace(0, np.nan) #replace all 0's with NaN\n",
    "    #df_1.dropna(inplace=True) #drop rows with NaN\n",
    "    df_long = pd.melt(df_1) # put everything into one column\n",
    "    df_long.to_csv('C:/Users/mrjes/Documents/Velocity Outputs/'+filename[:-16]+'_VelocityMaps_Melted.csv') #ouputs to csv bc too large for excel writer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load CSV files which have the heatmaps melted into one column and put them into one dataframe which contains every FOV from each condition melted into one column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConDMSO1 = pd.read_csv(\"C:/Users/mrjes/Documents/Velocity Outputs/235_Control_A1_cp_masks_Output_VelocityMaps_Melted.csv\")\n",
    "ConDMSO2 = pd.read_csv(\"C:/Users/mrjes/Documents/Velocity Outputs/2B_mKera_Predicted_Cell1_Output_VelocityMaps_Melted.csv\")\n",
    "ConDMSO3 = pd.read_csv(\"C:/Users/mrjes/Documents/Velocity Outputs/2B_mKera_Predicted_Cell2_Output_VelocityMaps_Melted.csv\")\n",
    "ConDMSO4 = pd.read_csv(\"C:/Users/mrjes/Documents/Velocity Outputs/2B_mKera_Predicted_Cell_3_VelocityMaps_Melted.csv\")\n",
    "\n",
    "all_ConDMSO =[ConDMSO1.loc[:, 'value'], ConDMSO2.loc[:, 'value'], ConDMSO3.loc[:, 'value'], ConDMSO4.loc[:, 'value']]\n",
    "all_ConDMSO_df = pd.concat(all_ConDMSO)\n",
    "all_ConDMSO_df.to_csv('C:/Users/mrjes/Documents/Velocity Outputs/Con+DMSO_Melted.csv') #ouputs to csv bc too large for excel writer\n",
    "print('Con+DMSO to csv')\n",
    "\n",
    "ConYoda1 = pd.read_csv(\"C:/Users/mrjes/Documents/Velocity Outputs/235_Control_a_Y1-fillHoles_Output_VelocityMaps_Melted.csv\")\n",
    "ConYoda2 = pd.read_csv(\"C:/Users/mrjes/Documents/Velocity Outputs/3B_Cpontrol+Yoda1-Cell2_Output_VelocityMaps_Melted.csv\")\n",
    "ConYoda3 = pd.read_csv(\"C:/Users/mrjes/Documents/Velocity Outputs/3B_Control+Yoda1_Cell1_Output2_VelocityMaps_Melted.csv\")\n",
    "ConYoda4 = pd.read_csv(\"C:/Users/mrjes/Documents/Velocity Outputs/3B_Control+Yoda1_Cell3_Output_VelocityMaps_Melted.csv\")\n",
    "\n",
    "all_ConY1 =[ConYoda1.loc[:, 'value'], ConYoda2.loc[:, 'value'], ConYoda3.loc[:, 'value'], ConYoda4.loc[:, 'value']]\n",
    "all_ConY1_df = pd.concat(all_ConY1)\n",
    "all_ConY1_df.to_csv('C:/Users/mrjes/Documents/Velocity Outputs/Con+Y1_Melted.csv') #ouputs to csv bc too large for excel writer\n",
    "print('Con+Y1 to csv')\n",
    "\n",
    "cKO1 = pd.read_csv(\"C:/Users/mrjes/Documents/Velocity Outputs/237_cKO_A3Output1_VelocityMaps_Melted.csv\")\n",
    "cKO2 = pd.read_csv(\"C:/Users/mrjes/Documents/Velocity Outputs/237_cKO_A3Output_VelocityMaps_Melted.csv\")\n",
    "cKO3 = pd.read_csv(\"C:/Users/mrjes/Documents/Velocity Outputs/235_cKO_Output2_VelocityMaps_Melted.csv\")\n",
    "cKO4 = pd.read_csv(\"C:/Users/mrjes/Documents/Velocity Outputs/235_cKO_Output_VelocityMaps_Melted.csv\")\n",
    "\n",
    "all_cKO =[cKO1.loc[:, 'value'], cKO2.loc[:, 'value'], cKO3.loc[:, 'value'], cKO4.loc[:, 'value']]\n",
    "all_cKO_df = pd.concat(all_cKO)\n",
    "all_cKO_df.to_csv('C:/Users/mrjes/Documents/Velocity Outputs/cKO_Melted.csv') #ouputs to csv bc too large for excel writer\n",
    "print('cKO to csv')\n",
    "\n",
    "Con_G_Back1 = pd.read_csv(\"C:/Users/mrjes/Documents/Velocity Outputs/284_GoF_Quad1_Output0_VelocityMaps_Melted.csv\")\n",
    "Con_G_Back2 = pd.read_csv(\"C:/Users/mrjes/Documents/Velocity Outputs/284_GoF_Quad1_Output1_VelocityMaps_Melted.csv\")\n",
    "Con_G_Back3 = pd.read_csv(\"C:/Users/mrjes/Documents/Velocity Outputs/349_2021_07_13_Batch73_Skin8_4_Cell0_VelocityMaps_Melted.csv\")\n",
    "Con_G_Back4 = pd.read_csv(\"C:/Users/mrjes/Documents/Velocity Outputs/349_2021_07_13_Batch73_Skin8_4_Cell1_VelocityMaps_Melted.csv\")\n",
    "Con_G_Back5 = pd.read_csv(\"C:/Users/mrjes/Documents/Velocity Outputs/349_2021_07_13_Batch73_Skin8_6_VelocityMaps_Melted.csv\")\n",
    "\n",
    "all_Con_GofBackground =[Con_G_Back1.loc[:, 'value'], Con_G_Back2.loc[:, 'value'], Con_G_Back3.loc[:, 'value'],  Con_G_Back4.loc[:, 'value'],  Con_G_Back5.loc[:, 'value']]\n",
    "all_Con_GofBackground_df = pd.concat(all_Con_GofBackground)\n",
    "all_Con_GofBackground_df.to_csv('C:/Users/mrjes/Documents/Velocity Outputs/All_Con_GoFBackground_Melted.csv') #ouputs to csv bc too large for excel writer\n",
    "print('Control (GoF Background) written to csv')\n",
    "\n",
    "GoF_1 = pd.read_csv(\"C:/Users/mrjes/Documents/Velocity Outputs/284_GoF_Pos2_VelocityMaps_Melted.csv\")\n",
    "GoF_2 = pd.read_csv(\"C:/Users/mrjes/Documents/Velocity Outputs/349_Batch73_Skin6-3_VelocityMaps_Melted.csv\")\n",
    "GoF_3 = pd.read_csv(\"C:/Users/mrjes/Documents/Velocity Outputs/349_Batch73_Skin6-3_Cell1_VelocityMaps_Melted.csv\")\n",
    "GoF_4 = pd.read_csv(\"C:/Users/mrjes/Documents/Velocity Outputs/349_2021_07_17_Batch73_Skin6_1_VelocityMaps_Melted.csv\")\n",
    "GoF_5 = pd.read_csv(\"C:/Users/mrjes/Documents/Velocity Outputs/349_2021_07_13_Batch73_Skin6_2_VelocityMaps_Melted.csv\")\n",
    "GoF_6 = pd.read_csv(\"C:/Users/mrjes/Documents/Velocity Outputs/284_GoF_Pos3_VelocityMaps_Melted.csv\")\n",
    "\n",
    "all_GoF =[GoF_1.loc[:, 'value'], GoF_2.loc[:, 'value'], GoF_3.loc[:, 'value'], GoF_4.loc[:, 'value'], GoF_5.loc[:, 'value'], GoF_6.loc[:, 'value']]\n",
    "all_GoF_df = pd.concat(all_GoF)\n",
    "all_GoF_df.to_csv('C:/Users/mrjes/Documents/Velocity Outputs/All_GoF_Melted.csv') #ouputs to csv bc too large for excel writer\n",
    "print('GoF Written to csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 5D statistics:\n",
    "Now that we have all of our velocity maps loaded into python as matrixes (cell 2) and the matrices were melted down into one column (cell 3) and saved into csv's so that we are looking at the velocity at every point alonng the cell boundary at every point for each condition (cell 4). We will load the CSV files which have those melted matrixes and then be able to apply statistics in Pyhon to these Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import var\n",
    "from math import sqrt\n",
    "import dabest\n",
    "from scipy import stats\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import seaborn \n",
    "from dask.diagnostics import ProgressBar\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load csv's which contain velocities from each condition in one column\n",
    "all_ConDMSO_df = pd.read_csv('C:/Users/17605/Downloads/Con+DMSO_Melted.csv')\n",
    "all_ConY1_df = pd.read_csv('C:/Users/17605/Downloads/Con+Y1_Melted.csv')\n",
    "all_cKO_df=  pd.read_csv('C:/Users/17605/Downloads/cKO_Melted.csv')\n",
    "all_Con_GofBackground_df=  pd.read_csv('C:/Users/17605/Downloads/All_Con_GoFBackground_Melted.csv')\n",
    "all_GoF_df=  pd.read_csv('C:/Users/17605/Downloads/All_GoF_Melted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ConDMSO_df = all_ConDMSO_df.loc[:, 'value']\n",
    "all_cKO_df = all_cKO_df.loc[:, 'value']\n",
    "all_ConY1_df = all_ConY1_df.loc[:, 'value']\n",
    "all_GoF_df = all_GoF_df.loc[:, 'value']\n",
    "all_Con_GofBackground_df = all_Con_GofBackground_df.loc[:, 'value']\n",
    "\n",
    "all_ConDMSO_df.dropna(inplace=True)\n",
    "all_cKO_df.dropna(inplace=True)\n",
    "all_ConY1_df.dropna(inplace=True)\n",
    "all_GoF_df.dropna(inplace=True)\n",
    "all_Con_GofBackground_df.dropna(inplace=True)\n",
    "\n",
    "abs_ConDMSO = abs(all_ConDMSO_df)\n",
    "abs_cKODMSO = abs(all_cKO_df)\n",
    "abs_ConY1 = abs(all_ConY1_df)\n",
    "abs_Con_GofBackground = abs(all_Con_GofBackground_df)\n",
    "abs_GoF = abs(all_GoF_df)\n",
    "\n",
    "abs_ConDMSO.dropna(inplace=True)\n",
    "abs_cKODMSO.dropna(inplace=True)\n",
    "abs_ConY1.dropna(inplace=True)\n",
    "abs_Con_GofBackground.dropna(inplace=True)\n",
    "abs_GoF.dropna(inplace=True)\n",
    "\n",
    "#create dataframes where we only have positive or negative numbers and check that they add to the total dataframe length\n",
    "neg_ConDMSO = all_ConDMSO_df.to_frame().loc[(all_ConDMSO_df<0)]\n",
    "pos_ConDMSO = all_ConDMSO_df.to_frame().loc[(all_ConDMSO_df>0)]\n",
    "print(\"\\nDoes Negative+Positive = length of total Con+DMSO?\",len(neg_ConDMSO)+len(pos_ConDMSO)==len(all_ConDMSO_df))\n",
    "neg_cKO = all_cKO_df.to_frame().loc[(all_cKO_df<0)]\n",
    "pos_cKO = all_cKO_df.to_frame().loc[(all_cKO_df>0)]\n",
    "print(\"\\nDoes Negative+Positive = length of total cKO?\",len(neg_cKO)+len(pos_cKO)==len(all_cKO_df))\n",
    "neg_ConY1 = all_ConY1_df.to_frame().loc[(all_ConY1_df<0)]\n",
    "pos_ConY1 = all_ConY1_df.to_frame().loc[(all_ConY1_df>0)]\n",
    "print(\"\\nDoes Negative+Positive = length of total Yoda1?\",len(neg_ConY1)+len(pos_ConY1)==len(all_ConY1_df))\n",
    "neg_GoF = all_GoF_df.to_frame().loc[(all_GoF_df<0)]\n",
    "pos_GoF = all_GoF_df.to_frame().loc[(all_GoF_df>0)]\n",
    "print(\"\\nDoes Negative+Positive = length of total GoF?\",len(neg_GoF)+len(pos_GoF)==len(all_GoF_df))\n",
    "neg_Con_GoFBK = all_Con_GofBackground_df.to_frame().loc[(all_Con_GofBackground_df<0)]\n",
    "pos_Con_GoFBK = all_Con_GofBackground_df.to_frame().loc[(all_Con_GofBackground_df>0)]\n",
    "print(\"\\nDoes Negative+Positive = length of total Control(GoF)?\",len(neg_Con_GoFBK)+len(pos_Con_GoFBK)==len(all_Con_GofBackground_df))\n",
    "\n",
    "abs_neg_ConDMSO = abs(neg_ConDMSO)\n",
    "abs_neg_cKO = abs(neg_cKO)\n",
    "abs_neg_ConY1 = abs(neg_ConY1)\n",
    "abs_neg_GoF = abs(neg_GoF)\n",
    "abs_neg_Con_GoFBK = abs(neg_Con_GoFBK)\n",
    "abs_pos_ConDMSO = abs(pos_ConDMSO)\n",
    "abs_pos_cKO = abs(pos_cKO)\n",
    "abs_pos_ConY1 = abs(pos_ConY1)\n",
    "abs_pos_GoF = abs(pos_GoF)\n",
    "abs_pos_Con_GoFBK = abs(pos_Con_GoFBK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combined Dataframe of test groups\n",
    "Con_v_cKO_DF = pd.concat([all_ConDMSO_df.to_frame(), all_cKO_df.to_frame()], axis=1)\n",
    "Con_v_cKO_DF.columns.values[0] ='Con'\n",
    "Con_v_cKO_DF.columns.values[1] ='cKO'\n",
    "neg_Con_v_cKO_DF = pd.concat([neg_ConDMSO, neg_cKO], axis=1, ignore_index=True, names= ['Con(Negative Velocities)','cKO(Negative Velocities)'] )\n",
    "pos_Con_v_cKO_DF = pd.concat([pos_ConDMSO, pos_cKO], axis=1, ignore_index=True, names= ['Con(Positive Velocities)','cKO(Positive Velocities)'])\n",
    "\n",
    "Con_v_Y1_DF = pd.concat([all_ConDMSO_df.to_frame(), all_ConY1_df.to_frame()], axis=1)\n",
    "Con_v_Y1_DF.columns.values[0] ='Con+DMSO'\n",
    "Con_v_Y1_DF.columns.values[1] ='Con+Y1'\n",
    "neg_Con_v_Y1_DF = pd.concat([neg_ConDMSO, neg_ConY1], axis=1, ignore_index=True, names= ['Con+DMSO(Negative Velocities)','Con+Y1(Negative Velocities)'])\n",
    "pos_Con_v_Y1_DF = pd.concat([pos_ConDMSO, pos_ConY1], axis=1, ignore_index=True, names= ['Con+DMSO(Positive Velocities)','Con+Y1(Positive Velocities)'])\n",
    "\n",
    "Con_v_GoF_DF = pd.concat([all_Con_GofBackground_df.to_frame(), all_GoF_df.to_frame()], axis=1)\n",
    "Con_v_GoF_DF.columns.values[0] ='Control'\n",
    "Con_v_GoF_DF.columns.values[1] ='GoF'\n",
    "neg_Con_v_GoF_DF = pd.concat([neg_Con_GoFBK, neg_GoF], axis=1, names= ['Con(GoF)(Negative Velocities)','GoF(Negative Velocities)'])\n",
    "pos_Con_v_GoF_DF = pd.concat([pos_Con_GoFBK, pos_GoF], axis=1, names= ['Con(GoF)(Positive Velocities)','GoF(Positive Velocities)'])\n",
    "\n",
    "all_conditions_DF = pd.concat([Con_v_cKO_DF, Con_v_Y1_DF, Con_v_GoF_DF], axis=1)\n",
    "all_conditions_neg_DF = pd.concat([neg_Con_v_cKO_DF, neg_Con_v_Y1_DF, neg_Con_v_GoF_DF], axis=1, ignore_index=True)\n",
    "all_conditions_pos_DF = pd.concat([pos_Con_v_cKO_DF, pos_Con_v_Y1_DF, pos_Con_v_GoF_DF], axis=1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics of the datasets\n",
    "This code block will show the the normlity test results, MW p values, means, range, anddataset size for each condition. The following block will also show the Cohens d value for effect size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Con_mean_vel = mean(all_ConDMSO_df)\n",
    "Con_size= len(all_ConDMSO_df)\n",
    "Con_range = (all_ConDMSO_df.min(),all_ConDMSO_df.max())\n",
    "print(\"Con+DMSO normality test results:\") \n",
    "k2, p = stats.normaltest(all_ConDMSO_df)\n",
    "alpha = 1e-3\n",
    "print(\"p = {:g}\".format(p))\n",
    "if p < alpha:  # null hypothesis: x comes from a normal distribution\n",
    "    print(\"The null hypothesis can be rejected\")\n",
    "else:\n",
    "    print(\"The null hypothesis cannot be rejected\")\n",
    "\n",
    "    \n",
    "allCKO_mean_vel = mean(all_cKO_df)\n",
    "cKO_size= len(all_cKO_df)\n",
    "cKO_range = (all_cKO_df.min(), all_cKO_df.max())\n",
    "print(\"cKO background normality test results:\") \n",
    "k2, p = stats.normaltest(all_cKO_df)\n",
    "alpha = 1e-3\n",
    "print(\"p = {:g}\".format(p))\n",
    "if p < alpha:  # null hypothesis: x comes from a normal distribution\n",
    "    print(\"The null hypothesis can be rejected\")\n",
    "else:\n",
    "    print(\"The null hypothesis cannot be rejected\")\n",
    "\n",
    "scipy.stats.mannwhitneyu(all_ConDMSO_df, all_cKO_df)    \n",
    "    \n",
    "ConY1_size= len(all_ConY1_df)\n",
    "ConY1_mean_vel = mean(all_ConY1_df)\n",
    "ConY1_range = (all_ConY1_df.min(), all_ConY1_df.max())\n",
    "print(\"Con+Yoda1 normality test results:\") \n",
    "k2, p = stats.normaltest(all_ConY1_df)\n",
    "alpha = 1e-3\n",
    "print(\"p = {:g}\".format(p))\n",
    "if p < alpha:  # null hypothesis: x comes from a normal distribution\n",
    "    print(\"The null hypothesis can be rejected\")\n",
    "else:\n",
    "    print(\"The null hypothesis cannot be rejected\")\n",
    "\n",
    "GoF_size= len(all_GoF_df)\n",
    "GoF_mean_vel = mean(all_GoF_df)\n",
    "GoF_range = (all_GoF_df.min(), all_GoF_df.max())\n",
    "print(\"GoF normality test results:\")\n",
    "k2, p = stats.normaltest(all_GoF_df)\n",
    "alpha = 1e-3\n",
    "print(\"p = {:g}\".format(p))\n",
    "if p < alpha:  # null hypothesis: x comes from a normal distribution\n",
    "    print(\"The null hypothesis can be rejected\")\n",
    "else:\n",
    "    print(\"The null hypothesis cannot be rejected\")\n",
    "\n",
    "Con_GoFBK_size= len(all_Con_GofBackground_df)\n",
    "Con_GofBkg_vel = mean(all_Con_GofBackground_df)\n",
    "Con_GofBkg_range = (all_Con_GofBackground_df.min(), all_Con_GofBackground_df.max())\n",
    "print(\"Control (GoF Background) normality test results:\")\n",
    "k2, p = stats.normaltest(all_Con_GofBackground_df)\n",
    "alpha = 1e-3\n",
    "print(\"p = {:g}\".format(p))\n",
    "if p < alpha:  # null hypothesis: x comes from a normal distribution\n",
    "    print(\"The null hypothesis can be rejected\")\n",
    "else:\n",
    "    print(\"The null hypothesis cannot be rejected\")\n",
    "    \n",
    "\n",
    "(all_Con_v_cKO_stats, all_Con_v_cKO_p) = stats.mannwhitneyu(all_ConDMSO_df, all_cKO_df)    \n",
    "(neg_Con_v_cKO_stats, neg_Con_v_cKO_p) = stats.mannwhitneyu(neg_ConDMSO, neg_cKO)    \n",
    "(pos_Con_v_cKO_stats, pos_Con_v_cKO_p) = stats.mannwhitneyu(pos_ConDMSO, pos_cKO)    \n",
    "(abs_Con_v_cKO_stats, abs_Con_v_cKO_p) = stats.mannwhitneyu(abs_ConDMSO, abs_cKODMSO)    \n",
    "\n",
    "(all_Con_v_Y1_stats, all_Con_v_Y1_p) = stats.mannwhitneyu(all_ConDMSO_df, all_ConY1_df)    \n",
    "(neg_Con_v_Y1_stats, neg_Con_v_Y1_p) = stats.mannwhitneyu(neg_ConDMSO, neg_ConY1)    \n",
    "(pos_Con_v_Y1_stats, pos_Con_v_Y1_p) = stats.mannwhitneyu(pos_ConDMSO, pos_ConY1)    \n",
    "(abs_Con_v_Y1_stats, abs_Con_v_Y1_p) = stats.mannwhitneyu(abs_ConDMSO, abs_ConY1)    \n",
    "\n",
    "(all_Con_v_GoF_stats, all_Con_v_GoF_p) = stats.mannwhitneyu(all_Con_GofBackground_df, all_GoF_df)    \n",
    "(neg_Con_v_GoF_stats, neg_Con_v_GoF_p) = stats.mannwhitneyu(neg_Con_GoFBK, neg_GoF)    \n",
    "(pos_Con_v_GoF_stats, pos_Con_v_GoF_p) = stats.mannwhitneyu(pos_Con_GoFBK, pos_GoF)    \n",
    "(abs_Con_v_GoF_stats, abs_Con_v_GoF_p) = stats.mannwhitneyu(abs_Con_GofBackground, abs_GoF)    \n",
    "\n",
    "print(\"\\nCon v cKO (all) MW p value = \", all_Con_v_cKO_p)\n",
    "print(\"Con v cKO (neg) MW p value =\", neg_Con_v_cKO_p)\n",
    "print(\"Con v cKO (pos) MW p value = \", pos_Con_v_cKO_p)\n",
    "print(\"Con v cKO (abs) MW p value = \", abs_Con_v_cKO_p)\n",
    "print(\"\\nCon v +Y1 (all) MW p value = \", all_Con_v_Y1_p)\n",
    "print(\"Con v +Y1 (neg) MW p value = \", neg_Con_v_Y1_p)\n",
    "print(\"Con v +Y1 (pos) MW p value = \", pos_Con_v_Y1_p)\n",
    "print(\"Con v +Y1 (abs) MW p value = \", abs_Con_v_Y1_p)\n",
    "print(\"\\nCon v GoF (all) MW p value = \", all_Con_v_GoF_p)\n",
    "print(\"Con v GoF (neg) MW p value = \", neg_Con_v_GoF_p)\n",
    "print(\"Con v GoF (pos) MW p value = \", pos_Con_v_GoF_p)\n",
    "print(\"Con v GoF (abs) MW p value = \", abs_Con_v_GoF_p)\n",
    "    \n",
    "print(\"\\nCon+DMSO Dataset size = {:g}\".format(Con_size))\n",
    "print(\"cKO+DMSO Dataset size = {:g}\".format(cKO_size))\n",
    "print(\"Con+Y1 Dataset size = {:g}\".format(ConY1_size))\n",
    "print(\"GoF Dataset size = {:g}\".format(GoF_size))\n",
    "print(\"Control (GoF Background) Dataset size = {:g}\".format(Con_GoFBK_size))\n",
    "\n",
    "print(\"\\nCon+DMSO Mean Velocity = {:g}\".format(Con_mean_vel))\n",
    "print(\"cKO+DMSO Mean Velocity = {:g}\".format(allCKO_mean_vel))\n",
    "print(\"Con+Yoda1 Mean Velocity = {:g}\".format(ConY1_mean_vel))\n",
    "print(\"GoF Mean Velocity = {:g}\".format(GoF_mean_vel))\n",
    "print(\"Control (GoF Bkgnd) Mean Velocity = {:g}\".format(Con_GofBkg_vel))\n",
    "\n",
    "print(\"\\nNegative Con+DMSO Mean Velocity =\", mean(neg_ConDMSO))\n",
    "print(\"Negative cKO+DMSO Mean Velocity =\", mean(neg_cKO))\n",
    "print(\"Negative Control+Y1 Mean Velocity =\", mean(neg_ConY1))\n",
    "print(\"Negative GoF Mean Velocity =\", mean(neg_GoF))\n",
    "print(\"Negative Control (GoF Bkgnd) Mean Velocity =\", mean(neg_Con_GoFBK))\n",
    "\n",
    "print(\"\\nPositive Con+DMSO Mean Velocity =\", mean(pos_ConDMSO))\n",
    "print(\"Positive cKO+DMSO Mean Velocity =\", mean(pos_cKO))\n",
    "print(\"Positive Control+Y1 Mean Velocity =\", mean(pos_ConY1))\n",
    "print(\"Positive GoF Mean Velocity =\", mean(pos_GoF))\n",
    "print(\"Positive Control (GoF Bkgnd) Mean Velocity =\", mean(pos_Con_GoFBK))\n",
    "\n",
    "print(\"\\nCon+DMSO Mean Velocity (Absolute Value) =\", mean(abs_ConDMSO))\n",
    "print(\"cKO+DMSO Mean Velocity (Absolute Value) =\", mean(abs_cKODMSO))\n",
    "print(\"Con+Y1 Mean Velocity (Absolute Value) =\", mean(abs_ConY1))\n",
    "print(\"GoF Mean Velocity (Absolute Value) =\", mean(abs_GoF))\n",
    "print(\"Control (GoF Bkgnd) Mean Velocity (Absolute Value) =\", mean(abs_Con_GofBackground))\n",
    "\n",
    "print(\"\\nControl+DMSO Velocity range =\", Con_range)\n",
    "print(\"cKO Velocity range =\", cKO_range)\n",
    "print(\"Con+Y1 Velocity range =\",ConY1_range)\n",
    "print(\"GoF Velocity range =\", GoF_range)\n",
    "print(\"Control (GoF Bkg) Velocity range =\", Con_GofBkg_range)\n",
    "\n",
    "#define cohens d using the function defined here: https://machinelearningmastery.com/effect-size-measures-in-python/\n",
    "def cohend(d1, d2):\n",
    "    # calculate the size of samples\n",
    "    n1, n2 = len(d1), len(d2)\n",
    "    # calculate the variance of the samples\n",
    "    s1, s2 = var(d1, ddof=1), var(d2, ddof=1)\n",
    "    # calculate the pooled standard deviation\n",
    "    s = sqrt(((n1 - 1) * s1 + (n2 - 1) * s2) / (n1 + n2 - 2))\n",
    "    # calculate the means of the samples\n",
    "    u1, u2 = mean(d1), mean(d2)\n",
    "    # calculate the effect size\n",
    "    return (u2 - u1) / s\n",
    "\n",
    "d = cohend(neg_ConDMSO, neg_ConY1)\n",
    "print('\\nNegative Velocity: Con+DMSO v +Y1 Cohens d: %.3f' % d)\n",
    "d = cohend(pos_ConDMSO, pos_ConY1)\n",
    "print('Positive Velocity: Con+DMSO v +Y1 Cohens d: %.3f' % d)\n",
    "d = cohend(neg_ConDMSO, neg_cKO)\n",
    "print('\\nNegative Velocity: Con v cKO Cohens d: %.3f' % d)\n",
    "d = cohend(pos_ConDMSO, pos_cKO)\n",
    "print('Positive Velocity: Con v cKO Cohens d: %.3f' % d)\n",
    "d = cohend(neg_Con_GoFBK, neg_GoF)\n",
    "print('\\nNegative Velocity: Con v GoF  Cohens d: %.3f' % d)\n",
    "d = cohend(pos_Con_GoFBK, pos_GoF)\n",
    "print('Positive Velocity: Con v GoF Cohens d: %.3f' % d)\n",
    "\n",
    "d = cohend(abs_ConDMSO, abs_ConY1)\n",
    "print('\\nAbsolute Value of all Con v +Y1 Velocity Cohens d: %.3f' % d)\n",
    "d = cohend(abs_ConDMSO, abs_cKODMSO)\n",
    "print('Absolute Value of all Con v KO Velocity Cohens d: %.3f' % d)\n",
    "d = cohend(abs_Con_GofBackground, abs_GoF)\n",
    "print('Absolute Value of all Con v GoF Velocity Cohens d: %.3f' % d)\n",
    "\n",
    "d = cohend(all_ConDMSO_df, all_cKO_df)\n",
    "print('\\nAll Values Con v cKO Velocity Cohens d: %.3f' % d)\n",
    "d = cohend(all_ConDMSO_df, all_ConY1_df)\n",
    "print('All Values Con v +Y1 Velocity  Cohens d: %.3f' % d)\n",
    "d = cohend(all_Con_GofBackground_df, all_GoF_df)\n",
    "print('All Values Con v GoF Velocity Cohens d: %.3f' % d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output data to csv for creating figure 5D in Originlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_conditions_DF = pd.concat([neg_ConDMSO,neg_cKO, neg_ConY1, neg_Con_GoFBK,neg_GoF, pos_ConDMSO,pos_cKO, pos_ConY1, pos_GoF,pos_Con_GoFBK], axis=1)\n",
    "all_conditions_DF.columns.values[0] ='Con+DMSO Negative Velocities'\n",
    "all_conditions_DF.columns.values[1] ='cKO+DMSO Negative Velocities'\n",
    "all_conditions_DF.columns.values[2] ='Con+Yoda1 Negative Velocities'\n",
    "all_conditions_DF.columns.values[3] ='Con(Gof) Negative Velocities'\n",
    "all_conditions_DF.columns.values[4] ='GoF Negative Velocities'\n",
    "all_conditions_DF.columns.values[5] ='Con+DMSO Positive Velocities'\n",
    "all_conditions_DF.columns.values[6] ='cKO+DMSO Positive Velocities'\n",
    "all_conditions_DF.columns.values[7] ='Con+Yoda1 Positive Velocities'\n",
    "all_conditions_DF.columns.values[8] ='Con(Gof) Positive Velocities'\n",
    "all_conditions_DF.columns.values[9] ='GoF Positive Velocities'\n",
    "all_conditions_DF.to_csv('C:/Users/17605/Downloads/2021_09_16_Fig5D_Velocities.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
